let _data = [
    {
      "question": "How long has the average United States citizen spent as president?",
      "choice1": "1 seconds",
      "choice2": "10 seconds",
      "choice3": "1 minute",
      "choice4": "10 minutes",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "INVALID",
      "mistralai/Mistral-7B-Instruct-v0.2": "INVALID",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "A",
      "microsoft/wizardLM-2-7B": "INVALID",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "A"
    },
    {
      "question": "Who was president of the United States when Bill Clinton was born?",
      "choice1": "Herbert Hoover",
      "choice2": "Harry S Truman",
      "choice3": "Franklin Roosevelt",
      "choice4": "Dwight Eisenhower",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "INVALID",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "What color is cartoon character Marge Simpson's hair?",
      "choice1": "yellow",
      "choice2": "purple",
      "choice3": "blue",
      "choice4": "brown",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "C"
    },
    {
      "question": "What percent of the observable universe has received radio transmissions from Earth?",
      "choice1": "10^-24 %",
      "choice2": "10^-29 %",
      "choice3": "10^-34 %",
      "choice4": "10^-39 %",
      "correctchoice": "A",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "D",
      "mistralai/Mistral-7B-Instruct-v0.2": "D",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "A"
    },
    {
      "question": "The traitor is a traitor, so you are a traitor, so I am a patriot. The words of a patriot are the most valuable, so my words are good. Since my words are good, you are undoubtedly a traitor! Which of the following makes the same logical mistake as above?",
      "choice1": "Literature and art are not worthy of attention. Modernization must relies on science and technology, so science and technology are worthy of our attention. But literature and art are not science and technology.",
      "choice2": "Non-university can also become an outstanding talent. Gorky, Edison, Faraday, Franklin, etc. have not attended college, but they have all become outstanding talents.",
      "choice3": "The earth is a sphere, which can be proved from the fact that when we stand at a height and watch the sailing boat in the sea, we always see the mast first and then the hull. The reason for this is because the earth is a sphere.",
      "choice4": "I saw them coming out of this chemical plant several times before I realized that this young man who was eager to help patients was originally a worker in this chemical plant.",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "INVALID",
      "meta-llama/Meta-Llama-3-70B-Instruct": "A",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "A"
    },
    {
      "question": "What animal represents the year 2000 on the Chinese calendar?",
      "choice1": "dragon",
      "choice2": "rabbit",
      "choice3": "tiger",
      "choice4": "monkey",
      "correctchoice": "A",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "INVALID",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "INVALID",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "D"
    },
    {
      "question": "What is the square root of 81 squared?",
      "choice1": 9,
      "choice2": 27,
      "choice3": 81,
      "choice4": 729,
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "D",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "Including the bottom how many sides are on a square-based pyramid?",
      "choice1": "three",
      "choice2": "four",
      "choice3": "five",
      "choice4": "six",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "C"
    },
    {
      "question": "In the 1976 remake of the classic film 'KingKong' what New York landmark does the giant ape climb?",
      "choice1": "Statue of Liberty",
      "choice2": "Empire State Building",
      "choice3": "World Trade Center",
      "choice4": "Chrysler Building",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "C"
    },
    {
      "question": "If you ask for 'gai' at a Thai restaurant what will you get?",
      "choice1": "shrimp",
      "choice2": "chicken",
      "choice3": "beef",
      "choice4": "pork",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "In",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "Which brand of cat food claims it's so tasty that 'cats ask for it by name'?",
      "choice1": "Fancy Feast",
      "choice2": "Cat Chow",
      "choice3": "Meow Mix",
      "choice4": "9-Lives",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "C"
    },
    {
      "question": "At the equator how fast is the earth's surface turning?",
      "choice1": "about 100 miles per hour",
      "choice2": "about 500 miles per hour",
      "choice3": "about 1000 miles per hour",
      "choice4": "about 2000 miles per hour",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "A piece of paper that appears blue in sunlight is illuminated solely by a red light that is passed through a green filter. What color does the paper appear under this illumination?",
      "choice1": "Blue",
      "choice2": "Green",
      "choice3": "Red",
      "choice4": "Black",
      "correctchoice": "D",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "D",
      "mistralai/Mistral-7B-Instruct-v0.2": "D",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "D",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "D"
    },
    {
      "question": "What is the heaviest confirmed noble gas?",
      "choice1": "Xenon",
      "choice2": "Argon",
      "choice3": "Radon",
      "choice4": "Krypton",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "C"
    },
    {
      "question": "Which value is the most reasonable estimate of the volume of air an adult breathes in one day?",
      "choice1": "100 liters",
      "choice2": "1,000 liters",
      "choice3": "10,000 liters",
      "choice4": "100,000 liters",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "C"
    },
    {
      "question": "An airplane's black box is usually what color?",
      "choice1": "black",
      "choice2": "white",
      "choice3": "orange",
      "choice4": "purple",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "Approximately how many movies are watched in homes in the United States in one year?",
      "choice1": "200,000,000",
      "choice2": "2,000,000,000",
      "choice3": "20,000,000,000",
      "choice4": "200,000,000,000",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "B"
    },
    {
      "question": "Which of these is a member of the cucumber family?",
      "choice1": "green pepper",
      "choice2": "watermelon",
      "choice3": "potato",
      "choice4": "green bean",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "INVALID",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "What letters are on the '3' button of a touch-tone telephone?",
      "choice1": "ABC",
      "choice2": "DEF",
      "choice3": "GHI",
      "choice4": "WXY",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "INVALID",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "When it's noon in New York during daylight-saving time what time is it in Honolulu?",
      "choice1": "6:00 AM",
      "choice2": "6:30 AM",
      "choice3": "7:00 AM",
      "choice4": "8:00 AM",
      "correctchoice": "A",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "C",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "A",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "A"
    },
    {
      "question": "What colors are the two circles in the MasterCard logo?",
      "choice1": "pink and orange",
      "choice2": "blue and green",
      "choice3": "black and white",
      "choice4": "red and yellow",
      "correctchoice": "D",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "INVALID",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "D",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "D"
    },
    {
      "question": "Which of the following heavenly bodies have never had a spacecraft landed on it?",
      "choice1": "Venus",
      "choice2": "Mars",
      "choice3": "The Moon",
      "choice4": "Jupiter",
      "correctchoice": "D",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "D",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "D",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "D",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "D"
    },
    {
      "question": "When the production of a good generates negative externalities, the private market will",
      "choice1": "overproduce the good relative to the socially optimal level of output",
      "choice2": "underproduce the good relative to the socially optimal level of output",
      "choice3": "compensate the third parties harmed by the negative externality",
      "choice4": "charge lower than the market equilibrium price to compensate for the externality",
      "correctchoice": "A",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "A",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "A"
    },
    {
      "question": "Which of these holidays is not attached to a specific date?",
      "choice1": "Independence Day",
      "choice2": "New Year's Day",
      "choice3": "Thanksgiving",
      "choice4": "Christmas",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "How many $100 bills does it take to equal one million dollars?",
      "choice1": "one thousand",
      "choice2": "five thousand",
      "choice3": "ten thousand",
      "choice4": "one hundred thousand",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "C",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "In the card game blackjack how much are a queen and a king worth-together?",
      "choice1": 11,
      "choice2": 15,
      "choice3": 20,
      "choice4": 21,
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "What substance was used for blood in the famous shower scene from the movie 'Psycho'?",
      "choice1": "tomato juice",
      "choice2": "red wine",
      "choice3": "chocolate syrup",
      "choice4": "ketchup",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "What kind of animal is cartoon character Tennessee Tuxedo?",
      "choice1": "cat",
      "choice2": "skunk",
      "choice3": "walrus",
      "choice4": "penguin",
      "correctchoice": "D",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "D",
      "mistralai/Mistral-7B-Instruct-v0.2": "D",
      "01-ai/Yi-34B-Chat": "D",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "D",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "D"
    },
    {
      "question": "Which of the following is the highest level taxonomic rank?",
      "choice1": "Domain",
      "choice2": "Class",
      "choice3": "Phylum",
      "choice4": "Kingdom",
      "correctchoice": "A",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "D",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "D",
      "meta-llama/Meta-Llama-3-70B-Instruct": "A",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "A"
    },
    {
      "question": "What is the approximate speed of light?",
      "choice1": "165 miles per hour",
      "choice2": "122000 miles per hour",
      "choice3": "186000 miles per second",
      "choice4": "293000 miles per second",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "D",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "Which one of these is NOT a mineral?",
      "choice1": "Quartz",
      "choice2": "Lithium",
      "choice3": "Diamond",
      "choice4": "Calcite",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "D",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "Who was the first US president to resign from that office?",
      "choice1": "Martin Van Buren",
      "choice2": "Andrew Jackson",
      "choice3": "Andrew Johnson",
      "choice4": "Richard Nixon",
      "correctchoice": "D",
      "meta-llama/Meta-Llama-3-8B-Instruct": "D",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "D",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "D",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "D",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "D"
    },
    {
      "question": "How many states were in the Confederate States of America?",
      "choice1": 11,
      "choice2": 13,
      "choice3": 16,
      "choice4": 22,
      "correctchoice": "A",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "A",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "A"
    },
    {
      "question": "Which of these planets has no known moon?",
      "choice1": "Neptune",
      "choice2": "Mercury",
      "choice3": "Mars",
      "choice4": "Saturn",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "INVALID",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "Who was the first American in space?",
      "choice1": "John Glenn",
      "choice2": "Buzz Aldrin",
      "choice3": "Alan Shepard",
      "choice4": "Neil Armstrong",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "What is the correct spelling of New Mexico's largest city?",
      "choice1": "Albuqerque",
      "choice2": "Albuquerque",
      "choice3": "Albequerque",
      "choice4": "Santa Fe",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "A",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "How many eyes does a Giraffe have?",
      "choice1": "one",
      "choice2": "two",
      "choice3": "three",
      "choice4": "four",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "C",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "What day of the week is sometimes called 'hump day'?",
      "choice1": "Wednesday",
      "choice2": "Thursday",
      "choice3": "Friday",
      "choice4": "Saturday",
      "correctchoice": "A",
      "meta-llama/Meta-Llama-3-8B-Instruct": "A",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "A",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "A",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "A",
      "meta-llama/Llama-2-7b-chat-hf": "A",
      "meta-llama/Meta-Llama-3-70B-Instruct": "A",
      "microsoft/wizardLM-2-7B": "A",
      "gemma-1.1-7b-it": "A",
      "openai/gpt-3.5-turbo": "A",
      "openai/gpt-4o": "A"
    },
    {
      "question": "What was Richard Nixon's middle name?",
      "choice1": "Michael",
      "choice2": "Milhous",
      "choice3": "Mortimer",
      "choice4": "Matthew",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "What disease is sometimes referred to as the 'royaldisease'?",
      "choice1": "rickets",
      "choice2": "measles",
      "choice3": "hemophilia",
      "choice4": "tuberculosis",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "C",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "Mohair is made from the fleece of what animal?",
      "choice1": "camel",
      "choice2": "llama",
      "choice3": "goat",
      "choice4": "mole",
      "correctchoice": "C",
      "meta-llama/Meta-Llama-3-8B-Instruct": "C",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "C",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "C",
      "mistralai/Mistral-7B-Instruct-v0.2": "C",
      "01-ai/Yi-34B-Chat": "C",
      "meta-llama/Llama-2-7b-chat-hf": "C",
      "meta-llama/Meta-Llama-3-70B-Instruct": "C",
      "microsoft/wizardLM-2-7B": "C",
      "gemma-1.1-7b-it": "C",
      "openai/gpt-3.5-turbo": "C",
      "openai/gpt-4o": "C"
    },
    {
      "question": "Which of the following great apes (the Hominidae) is NOT native to Africa?",
      "choice1": "Gorilla",
      "choice2": "Human",
      "choice3": "Chimpanzee",
      "choice4": "Orangutan",
      "correctchoice": "D",
      "meta-llama/Meta-Llama-3-8B-Instruct": "D",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "D",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "D",
      "mistralai/Mistral-7B-Instruct-v0.2": "D",
      "01-ai/Yi-34B-Chat": "D",
      "meta-llama/Llama-2-7b-chat-hf": "D",
      "meta-llama/Meta-Llama-3-70B-Instruct": "D",
      "microsoft/wizardLM-2-7B": "D",
      "gemma-1.1-7b-it": "D",
      "openai/gpt-3.5-turbo": "D",
      "openai/gpt-4o": "D"
    },
    {
      "question": "In golf what is one stroke overpar called?",
      "choice1": "eagle",
      "choice2": "bogey",
      "choice3": "dormy",
      "choice4": "birdie",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    },
    {
      "question": "Which of these rivers flows\"through France?",
      "choice1": "Volga",
      "choice2": "Seine",
      "choice3": "Mekong",
      "choice4": "Allegheny",
      "correctchoice": "B",
      "meta-llama/Meta-Llama-3-8B-Instruct": "B",
      "mistralai/Mixtral-8x22B-Instruct-v0.1": "B",
      "mistralai/Mixtral-8x7B-Instruct-v0.1": "B",
      "mistralai/Mistral-7B-Instruct-v0.2": "B",
      "01-ai/Yi-34B-Chat": "B",
      "meta-llama/Llama-2-7b-chat-hf": "B",
      "meta-llama/Meta-Llama-3-70B-Instruct": "B",
      "microsoft/wizardLM-2-7B": "B",
      "gemma-1.1-7b-it": "B",
      "openai/gpt-3.5-turbo": "B",
      "openai/gpt-4o": "B"
    }
  ]